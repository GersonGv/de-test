{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d58472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Admin\\\\BigData\\\\Spark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07129fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "spark = SparkSession.builder.appName('etl').getOrCreate()  # I initilize the spark session \"spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74819f9d-5d8b-4982-9c89-333f13aafad0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating order_items_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b438775f-10d4-4337-9908-be774aa03221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " order_id      | e67e26c2-a04d-4ec... \n",
      " name          | 403 Mariano Proc√≥pio \n",
      " addition      | {\"value\":\"0\",\"cur... \n",
      " discount      | {\"value\":\"0\",\"cur... \n",
      " quantity      | 1.0                  \n",
      " sequence      | 1                    \n",
      " unitPrice     | {\"value\":\"0\",\"cur... \n",
      " externalId    | ee0f88c318af46129... \n",
      " totalValue    | {\"value\":\"0\",\"cur... \n",
      " customerNote  | null                 \n",
      " garnishItems  | [{\"name\":\"AO PONT... \n",
      " integrationId | null                 \n",
      " totalAddition | {\"value\":\"0\",\"cur... \n",
      " totalDiscount | {\"value\":\"0\",\"cur... \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "part_df = spark.read.json(r'C:\\Users\\Admin\\Documents\\Spark\\test\\part.json\\part.json')\n",
    "\n",
    "\n",
    "df_new = part_df.select('order_id', 'items')\n",
    "df = df_new.withColumn('col', from_json(\"items\", ArrayType(StringType())))\n",
    "df = df.withColumn('explode_col', explode(\"col\"))\n",
    "df = df.withColumn('col', from_json(\"explode_col\", MapType(StringType(), StringType())))\n",
    "df = df.withColumn(\"name\", df.col.getItem(\"name\")).withColumn(\"addition\", df.col.getItem(\"addition\")).withColumn(\"discount\", df.col.getItem(\"discount\")).withColumn(\"quantity\", df.col.getItem(\"quantity\")).withColumn(\"sequence\", df.col.getItem(\"sequence\")).withColumn(\"unitPrice\", df.col.getItem(\"unitPrice\")).withColumn(\"externalId\", df.col.getItem(\"externalId\")).withColumn(\"totalValue\", df.col.getItem(\"totalValue\")).withColumn(\"customerNote\", df.col.getItem(\"customerNote\")).withColumn(\"garnishItems\", df.col.getItem(\"garnishItems\")).withColumn(\"integrationId\", df.col.getItem(\"integrationId\")).withColumn(\"totalAddition\", df.col.getItem(\"totalAddition\")).withColumn(\"totalDiscount\", df.col.getItem(\"totalDiscount\"))\n",
    "order_item_dataset = df.select('order_id', 'name', 'addition', 'discount', 'quantity', 'sequence', 'unitPrice', 'externalId', 'totalValue', 'customerNote', 'garnishItems', 'integrationId', 'totalAddition', 'totalDiscount')\n",
    "order_item_dataset.show(1, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335448f-db09-44d1-98b9-1cbb238da893",
   "metadata": {},
   "source": [
    "### Creating order_status_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2802aae9-f8b9-4861-8b2b-7db9e42869d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+---------+\n",
      "|            order_id|             PLACED|         REGISTERED|          CONCLUDED|CANCELLED|\n",
      "+--------------------+-------------------+-------------------+-------------------+---------+\n",
      "|0002fe02-d7dc-423...|2019-01-24 17:04:28|2019-01-24 17:04:27|2019-01-24 19:05:07|     null|\n",
      "|000cef8c-83c7-49e...|2019-01-17 16:42:18|2019-01-17 16:42:17|2019-01-17 18:45:02|     null|\n",
      "|0010995b-9212-455...|2019-01-01 16:11:22|2019-01-01 16:11:21|2019-01-01 18:15:14|     null|\n",
      "|0012d95c-9c4b-424...|2019-01-03 12:12:24|2019-01-03 12:12:23|2019-01-03 14:15:06|     null|\n",
      "|0013fc5c-4c10-440...|2019-01-06 08:16:18|2019-01-06 08:16:17|2019-01-06 10:20:27|     null|\n",
      "+--------------------+-------------------+-------------------+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = \"`created_at` timestamp, `order_id` STRING, `status_id` STRING, `value` STRING\"\n",
    "status = spark.read.schema(schema).json(r'C:\\Users\\Admin\\Documents\\Spark\\test\\status.json\\status.json')\n",
    "\n",
    "col = ['PLACED', 'REGISTERED', 'CONCLUDED', 'CANCELLED']\n",
    "out_df = status.groupby(\"order_id\").pivot(\"value\", col).agg(first(\"created_at\"))\n",
    "out_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431fc3d-2d01-49ea-b52f-3f165fed3b7e",
   "metadata": {},
   "source": [
    "### Creating order_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea54c6d-4219-4d4a-851c-c5c0fc5b0dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------\n",
      " cpf                          | 23513081815          \n",
      " customer_id                  | 42b790bb-1499-4d2... \n",
      " customer_name                | JULIO                \n",
      " delivery_address_city        | RECIFE               \n",
      " delivery_address_country     | BR                   \n",
      " delivery_address_district    | AREIAS               \n",
      " delivery_address_external_id | 7612816              \n",
      " delivery_address_latitude    | -34.94               \n",
      " delivery_address_longitude   | -8.10                \n",
      " delivery_address_state       | PE                   \n",
      " delivery_address_zip_code    | 50780                \n",
      " items                        | [{\"name\": \"125 - ... \n",
      " merchant_id                  | 1365cc91-546a-4a9... \n",
      " merchant_latitude            | -34.94               \n",
      " merchant_longitude           | -8.10                \n",
      " merchant_timezone            | America/Recife       \n",
      " order_created_at             | 2019-01-01T22:11:... \n",
      " order_id                     | 0010995b-9212-455... \n",
      " order_scheduled              | false                \n",
      " order_scheduled_date         | null                 \n",
      " order_total_amount           | 41.4                 \n",
      " origin_platform              | IOS                  \n",
      " id                           | 1365cc91-546a-4a9... \n",
      " created_at                   | 2017-01-20T13:13:... \n",
      " enabled                      | true                 \n",
      " price_range                  | 2                    \n",
      " average_ticket               | 40.0                 \n",
      " takeout_time                 | 0                    \n",
      " delivery_time                | 0                    \n",
      " minimum_order_value          | 0.0                  \n",
      " merchant_zip_code            | 50910                \n",
      " merchant_city                | RECIFE               \n",
      " merchant_state               | PE                   \n",
      " merchant_country             | BR                   \n",
      " customer_id                  | 42b790bb-1499-4d2... \n",
      " language                     | pt-br                \n",
      " created_at                   | 2018-02-03T22:57:... \n",
      " active                       | true                 \n",
      " customer_name                | JULIO                \n",
      " customer_phone_area          | 28                   \n",
      " customer_phone_number        | 701701444            \n",
      " created_at                   | 2019-01-01 18:15:14  \n",
      " order_id                     | 0010995b-9212-455... \n",
      " status_id                    | ccadfe99-b614-412... \n",
      " value                        | CONCLUDED            \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "restaurant = spark.read.csv(r'C:\\Users\\Admin\\Documents\\Spark\\test\\restaurant.csv\\restaurant.csv', header=True)\n",
    "consumer = spark.read.csv(r'C:\\Users\\Admin\\Documents\\Spark\\test\\consumer.csv\\consumer.csv', header=True)\n",
    "\n",
    "# Creating a status dataset with only the last status value\n",
    "w = Window.partitionBy(status['order_id']).orderBy(status['created_at'].desc())\n",
    "status = status.withColumn('Rank', dense_rank().over(w))\n",
    "status_new = status.filter(status.Rank == 1).drop(status.Rank)\n",
    "\n",
    "order_dataset = part_df.join(restaurant, part_df[\"merchant_id\"] == restaurant['id']).join(consumer, part_df['customer_id'] == consumer['customer_id']).join(status_new, part_df[\"order_id\"] == status_new['order_id'])\n",
    "order_dataset.show(1, vertical = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
